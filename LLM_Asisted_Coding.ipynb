{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('../csv/DataSet_Hashed.csv')\n",
    "print (dataset.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56216c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf36b42",
   "metadata": {},
   "source": [
    "#First phase of coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee126375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- MODIFIED: Hardened, Multi-Stage JSON Extraction Function ---\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    Finds and loads the first valid JSON object from a string.\n",
    "    Priority 1: Look for content within <json_output> delimiters.\n",
    "    Priority 2: Fallback to finding the outermost JSON object {}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Priority 1: Look for content within <json_output> tags\n",
    "        start_tag = \"<json_output>\"\n",
    "        end_tag = \"</json_output>\"\n",
    "        tag_start_index = text.find(start_tag)\n",
    "        tag_end_index = text.rfind(end_tag)\n",
    "\n",
    "        if tag_start_index != -1 and tag_end_index != -1:\n",
    "            json_str = text[tag_start_index + len(start_tag):tag_end_index].strip()\n",
    "            return json.loads(json_str)\n",
    "\n",
    "        # Priority 2: Fallback to finding the first and last curly brace\n",
    "        start_index = text.find('{')\n",
    "        end_index = text.rfind('}')\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_str = text[start_index:end_index+1]\n",
    "            return json.loads(json_str)\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        # This error message is crucial for debugging.\n",
    "        print(f\"    Could not parse JSON from response. Error: {e}\")\n",
    "        print(f\"    Raw Text Received: {text}\") # Print the raw text that failed\n",
    "        return None\n",
    "    \n",
    "    # If no JSON is found at all, return None\n",
    "    return None\n",
    "\n",
    "# --- MODIFIED: Core analysis function with retry logic and hardened prompt ---\n",
    "def analyze_tiktok_video(row, headers):\n",
    "    \"\"\"\n",
    "    Analyzes a single row of TikTok data with a hardened prompt and retry logic.\n",
    "    \"\"\"\n",
    "    codebook = \"\"\"\n",
    "    \n",
    "   ## Codebook of the Qualitative Content Analysis ##\n",
    "\n",
    "    **Category: Journey Narratives**\n",
    "    - Code: JN-TE, Label: Transformative Experiences, Description: Self-actualization through travel.\n",
    "    - Code: JN-EP, Label: Episodic Progress, Descrition: Narrative of experiences, milestones while traveling.\n",
    "    - Code: JN-PG, Label: Personal Growth, Description: Insights and achievements through travel.\n",
    "\n",
    "    **Category: Digital Representation**\n",
    "    - Code: DR-VS, Label: Visual Storytelling, Description: Emphasize visual appeal, e.g., coastal landscapes, city lines, and airplane flights.\n",
    "    - Code: DR-IE, Label: Idealization of Experiences, Description: The idealized version of the future, career, and destinations.\n",
    "    - Code: DR-CA, Label: Cultural Authenticity, Description: The code showcases local experiences, cultural appropriation, or superficial representation.\n",
    "    - Code: DR-PSC, Label: Platform-Specific Content, Description: Content formatted according to TikTok grammar and logic.\n",
    "\n",
    "    **Category: Commercialization**\n",
    "    - Code: CA-TC, Label: Tourism and Commerce, Description: Destination promotion content may contribute to over-tourism issues.\n",
    "    - Code: CA-PC, Label: Promotional Content, Description: Affiliated links or product placement in the content.\n",
    "\n",
    "    **Category: Symbolism**\n",
    "    - Code: SY-SL, Label: Symbolic Imagery, Description: Symbolic representation: e.g., women in bikinis represent leisure activities.\n",
    "    - Code: SY-RB, Label: Rituals, Description: The code represents local traditions, e.g., Muslim broadcasting of psalms at 6 a.m.\n",
    "\n",
    "    **Category: Existential Themes**\n",
    "    - Code: ET-SD, Label: Self-Discovery, Description: Knowledge acquisition, discovery.\n",
    "    - Code: ET-PM, Label: Purpose and Meaning, Description: Finding a meaning in the activity.\n",
    "    - Code: ET-SG, Label: Spiritual Growth, Description: The notion of becoming a better version of oneself.\n",
    "    - Code: ET-IC, Label: Identity Construction, Description: Narratives that aim to construct identity online.\n",
    "    - Code: ET-PF, Label: Personal & Spatial Freedom, Description: The freedom through mobility, the perception that the whole world is open for exploration.\n",
    "\n",
    "    **Category: Hedonism**\n",
    "    - Code: HE-SA, Label: Sport Activities, Description: The active type of recreational consumption, e.g., surfing, cycling, paragliding.\n",
    "    - Code: HE-CE, Label: Culinary Experiences, Description: Projection of food consumption, preparation, or purchase.\n",
    "    - Code: HE-SE, Label: Shared Cultural Experiences & Bonding, Description: Experiences of pleasure and sensory gratification include festivals, carnivals, and performances.\n",
    "\n",
    "    **Category: TikTok Affordances**\n",
    "    - Code: TA-AC, Label: Accessibility (e.g., templates), Description: Ease of expression and re-use of content.\n",
    "    - Code: TA-DI, Label: Discoverability (e.g., hashtags, For You Page, audios), Description: Features that enhance content discoverability, enabling the reach of wider audiences beyond immediate followers.\n",
    "    - Code: TA-RL, Label: Relatability (trends, challenges), Description: Dialogic content that resonates with and encourages viewer self-identification and participation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The system prompt is now highly structured to guide the model.\n",
    "    system_message_content = f\"\"\"\n",
    "    You are a qualitative coder. Your task is to analyze the provided TikTok data and return a single, valid JSON object.\n",
    "\n",
    "    **Instructions:**\n",
    "    1.  Carefully review the Content Description, Hashtags, and Captions below.\n",
    "    2.  Based on the Codebook provided, determine the most relevant codes (up to 5).\n",
    "    3.  Formulate a brief `reason` that explains and justifies your code choices.\n",
    "    4.  Your final output MUST be ONLY a valid JSON object enclosed within `<json_output>` and `</json_output>` delimiters. Do NOT include any other text, markdown, or explanations like `<think>` outside these delimiters.\n",
    "\n",
    "    **Codebook:**\n",
    "    {codebook}\n",
    "    \"\"\"\n",
    "    \n",
    "    # The user prompt uses the \"sandwich\" technique, placing the data between instructions.\n",
    "    user_prompt = f\"\"\"\n",
    "    Analyze the following data based on the system instructions:\n",
    "\n",
    "    **Input Data:**\n",
    "    - Content Description: {row.get('desc', 'N/A')}\n",
    "    - Hashtags: {row.get('hashtagNames', 'N/A')}\n",
    "    - Captions: {row.get('stickersText', 'N/A')}\n",
    "\n",
    "    Remember, provide your response ONLY as a JSON object wrapped in <json_output> tags.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message_content}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    payload = {\"model\": \"sonar-reasoning\", \"messages\": messages, \"max_tokens\": 2048, \"temperature\": 0.1}\n",
    "\n",
    "    # The robust retry logic remains.\n",
    "    max_retries = 3\n",
    "    base_delay = 2\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\"https://api.perplexity.ai/chat/completions\", headers=headers, json=payload, timeout=60)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                raw_output = response.json()['choices'][0]['message']['content']\n",
    "                return extract_json_from_text(raw_output)\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                delay = (attempt + 1) * 10\n",
    "                print(f\"    Rate limit hit. Waiting {delay}s before retrying...\")\n",
    "                time.sleep(delay)\n",
    "            elif 500 <= response.status_code < 600:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"    Server Error {response.status_code}. Retrying in {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"    Client/API Error {response.status_code}: {response.text}\")\n",
    "                return None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            print(f\"    Request Exception: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    print(f\"    All {max_retries} retries failed for Video ID {row.get('hashed_videoId', 'N/A')}.\")\n",
    "    return None\n",
    "\n",
    "# --- Main execution block with CHECKPOINTING ---\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    sonar_key = os.getenv(\"SONAR_KEY\")\n",
    "    if not sonar_key: raise ValueError(\"Error: SONAR_KEY not found in .env file.\")\n",
    "    \n",
    "    print(\"SONAR_KEY loaded successfully.\")\n",
    "    headers = {\"Authorization\": f\"Bearer {sonar_key}\", \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    output_json_file = 'analysis_results.json'\n",
    "    failed_ids_file = 'failed_videos.txt'\n",
    "    \n",
    "    try:\n",
    "        text_cols = ['desc', 'hashtagNames', 'stickersText']\n",
    "        for col in text_cols:\n",
    "            if col in dataset.columns:\n",
    "                dataset[col] = dataset[col].fillna('')\n",
    "        \n",
    "       # Checkpointing logic to resume from where you left off\n",
    "        successful_results = []\n",
    "        processed_ids = set()\n",
    "\n",
    "        if os.path.exists(output_json_file):\n",
    "            print(f\"Found existing results file: '{output_json_file}'. Resuming...\")\n",
    "            try:\n",
    "                with open(output_json_file, 'r', encoding='utf-8') as f:\n",
    "                    successful_results = json.load(f)\n",
    "                    processed_ids = {item['hashed_videoId'] for item in successful_results if 'hashed_videoId' in item}\n",
    "                print(f\"Loaded {len(successful_results)} previously processed videos.\")\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                print(\"Warning: Could not read existing results file. Starting from scratch.\")\n",
    "                successful_results = []\n",
    "                processed_ids = set()\n",
    "\n",
    "        all_rows = dataset.to_dict(orient='records')\n",
    "        rows_to_process = [row for row in all_rows if row['hashed_videoId'] not in processed_ids]\n",
    "        \n",
    "        if not rows_to_process:\n",
    "            print(\"All videos have already been processed. Exiting.\")\n",
    "        else:\n",
    "            print(f\"\\n--- Starting analysis on {len(rows_to_process)} remaining videos ---\")\n",
    "            \n",
    "            for i, row in enumerate(rows_to_process):\n",
    "                print(f\"Processing {i+1}/{len(rows_to_process)} - Video ID: {row['hashed_videoId']}\")\n",
    "                result = analyze_tiktok_video(row, headers)\n",
    "                \n",
    "                if result:\n",
    "                    result['hashed_videoId'] = row['hashed_videoId']\n",
    "                    successful_results.append(result)\n",
    "                    # Checkpoint every 50 results\n",
    "                    if (i + 1) % 50 == 0:\n",
    "                        print(f\"    Checkpointing... saving {len(successful_results)} results.\")\n",
    "                        with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(successful_results, f, ensure_ascii=False, indent=4)\n",
    "                else:\n",
    "                    print(f\"    *** FAILED to process Video ID: {row['hashed_videoId']} ***\")\n",
    "                    with open(failed_ids_file, 'a', encoding='utf-8') as f:\n",
    "                        f.write(f\"{row['hashed_videoId']}\\n\")\n",
    "                        %store failed_ids_file\n",
    "\n",
    "        print(\"\\n--- Finalizing Results ---\")\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(successful_results, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"\\n--- Script finished ---\")\n",
    "        print(f\"Total successfully processed: {len(successful_results)} videos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during main execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b530e",
   "metadata": {},
   "source": [
    "#Second phase of coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    Finds and loads the first valid JSON object from a string,\n",
    "    looking for content within <json_output> delimiters if present.\n",
    "    Falls back to curly brace matching if delimiters are not found or invalid.\n",
    "    \"\"\"\n",
    "    start_tag = \"<json_output>\"\n",
    "    end_tag = \"</json_output>\"\n",
    "    tag_start_index = text.find(start_tag)\n",
    "    tag_end_index = text.rfind(end_tag)\n",
    "\n",
    "    if tag_start_index != -1 and tag_end_index != -1:\n",
    "        json_str_candidate = text[tag_start_index + len(start_tag):tag_end_index].strip()\n",
    "        try:\n",
    "            return json.loads(json_str_candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Priority 2: Fallback to finding the first and last curly brace\n",
    "    try:\n",
    "        start_index = text.find('{')\n",
    "        end_index = text.rfind('}')\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_str = text[start_index:end_index+1]\n",
    "            return json.loads(json_str)\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        print(f\"Could not parse JSON from response: {text}, Error: {e}\")\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def analyze_tiktok_video_element(row, headers):\n",
    "    \"\"\"\n",
    "    Performs second-step coding using a defined framework and a dedicated LLM.\n",
    "    Makes direct API calls using requests with provided headers.\n",
    "    \"\"\"\n",
    "    system_message_content = \"\"\"\n",
    "    You are a qualitative coder performing a second-order classification. Your task is to analyze a set of pre-assigned codes from a TikTok video and map them to a single 'Element' and its corresponding 'Primary Need' from the provided framework. \n",
    "    Important: Regardless of the input language, acknowledge the language but analyze in English using the same framework.\n",
    "\n",
    "    **## THEORETICAL FRAMEWORK ##**\n",
    "\n",
    "    You must use the following Hierarchy of Needs for each element:\n",
    "\n",
    "    {\n",
    "        \"Worker\": {\"Basic\": \"Location independence, internet connection\", \"Safety\": \"Diversification of income, stable work environment\", \"Social\": \"Professional networking, co-working spaces\", \"Esteem\": \"Career growth, skill development\", \"Self-actualization\": \"Work-life balance, meaningful work\"},\n",
    "        \"Tourist\": {\"Basic\": \"Accommodation, food\", \"Safety\": \"Comfortable and affordable travel\", \"Social\": \"Meeting locals and other travelers\", \"Esteem\": \"Unique experiences, cultural immersion\", \"Self-actualization\": \"Personal growth through travel\"},\n",
    "        \"Migrant\": {\"Basic\": \"Legal status, housing\", \"Safety\": \"Understanding local laws, healthcare access\", \"Social\": \"Integration into the local community\", \"Esteem\": \"Cultural adaptation, language skills\", \"Self-actualization\": \"Sense of belonging in a new place\"},\n",
    "        \"Pilgrim\": {\"Basic\": \"Finding purpose in travel\", \"Safety\": \"Spiritual or emotional security\", \"Social\": \"Connecting with like-minded individuals\", \"Esteem\": \"Personal transformation\", \"Self-actualization\": \"Achieving life goals, enlightenment\"}\n",
    "    }\n",
    "\n",
    "    **## YOUR TASK ##**\n",
    "\n",
    "    Based on the input codes provided by the user, produce a single JSON object as output. Your output MUST be enclosed within `<json_output>` and `</json_output>` delimiters.\n",
    "\n",
    "    The JSON must contain:\n",
    "    - \"reasoning\": A concise explanation for why you chose the element, based on the input codes.\n",
    "    - \"element\": One of the four elements (Worker, Tourist, Migrant, Pilgrim).\n",
    "    - \"need\": The primary need (Basic, Safety, Social, Esteem, Self-actualization) demonstrated in the content.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Analyze the following codes and provide your classification.\n",
    "    Reason: {row.get('reason', 'N/A')}\n",
    "    Input Codes: {row.get('codes', 'N/A')}\n",
    "    Video ID: {row.get('hashed_videoId', 'N/A')}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message_content},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"sonar-reasoning\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "\n",
    "    \n",
    "    # The robust retry logic remains.\n",
    "    max_retries = 3\n",
    "    base_delay = 2\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\"https://api.perplexity.ai/chat/completions\", headers=headers, json=payload, timeout=60)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                raw_output = response.json()['choices'][0]['message']['content']\n",
    "                return extract_json_from_text(raw_output)\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                delay = (attempt + 1) * 10\n",
    "                print(f\"    Rate limit hit. Waiting {delay}s before retrying...\")\n",
    "                time.sleep(delay)\n",
    "            elif 500 <= response.status_code < 600:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"    Server Error {response.status_code}. Retrying in {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"    Client/API Error {response.status_code}: {response.text}\")\n",
    "                return None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            print(f\"    Request Exception: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    print(f\"    All {max_retries} retries failed for Video ID {row.get('hashed_videoId', 'N/A')}.\")\n",
    "    return None\n",
    "\n",
    "def process_analysis_results(file_path):\n",
    "    \"\"\"\n",
    "    Reads the analysis_results.json file and returns the data as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from {file_path}.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main execution block with CHECKPOINTING ---\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    sonar_key = os.getenv(\"SONAR_KEY\")\n",
    "    if not sonar_key: raise ValueError(\"Error: SONAR_KEY not found in .env file.\")\n",
    "    \n",
    "    print(\"SONAR_KEY loaded successfully.\")\n",
    "    headers = {\"Authorization\": f\"Bearer {sonar_key}\", \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    output_json_file = 'needs_results.json'\n",
    "    failed_ids_file = 'failed_needs_ids.txt'\n",
    "    \n",
    "    try:\n",
    "        # Checkpointing logic to resume from where you left off\n",
    "        successful_results = []\n",
    "        processed_ids = set()\n",
    "        \n",
    "        if os.path.exists(output_json_file):\n",
    "            print(f\"Found existing results file: '{output_json_file}'. Resuming...\")\n",
    "            try:\n",
    "                with open(output_json_file, 'r', encoding='utf-8') as f:\n",
    "                    successful_results = json.load(f)\n",
    "                    processed_ids = {item['hashed_videoId'] for item in successful_results if 'hashed_videoId' in item}\n",
    "                print(f\"Loaded {len(successful_results)} previously processed videos.\")\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                print(\"Warning: Could not read existing results file. Starting from scratch.\")\n",
    "                successful_results = []\n",
    "                processed_ids = set()\n",
    "        \n",
    "        # Load the dataset from analysis_results.json\n",
    "        all_rows = process_analysis_results('analysis_results.json')\n",
    "        rows_to_process = [row for row in all_rows if row.get('hashed_videoId', 'N/A') not in processed_ids]\n",
    "        \n",
    "        if not rows_to_process:\n",
    "            print(\"All videos have already been processed. Exiting.\")\n",
    "        else:\n",
    "            print(f\"\\n--- Starting analysis on {len(rows_to_process)} remaining videos ---\")\n",
    "            \n",
    "            for i, row in enumerate(rows_to_process):\n",
    "                print(f\"Processing {i+1}/{len(rows_to_process)} - Video ID: {row.get('hashed_videoId', 'N/A')}\")\n",
    "                result = analyze_tiktok_video_element(row, headers)\n",
    "                \n",
    "                if result:\n",
    "                    result['hashed_videoId'] = row.get('hashed_videoId', 'N/A')\n",
    "                    successful_results.append(result)\n",
    "                    # Checkpoint every 50 results\n",
    "                    if (i + 1) % 50 == 0:\n",
    "                        print(f\"    Checkpointing... saving {len(successful_results)} results.\")\n",
    "                        with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(successful_results, f, ensure_ascii=False, indent=4)\n",
    "                else:\n",
    "                    print(f\"    *** FAILED to process Video ID: {row.get('hashed_videoId', 'N/A')} ***\")\n",
    "                    with open(failed_ids_file, 'a', encoding='utf-8') as f:\n",
    "                        f.write(f\"{row.get('hashed_videoId', 'N/A')}\\n\")\n",
    "        \n",
    "        print(\"\\n--- Finalizing Results ---\")\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(successful_results, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"\\n--- Script finished ---\")\n",
    "        print(f\"Total successfully processed: {len(successful_results)} videos.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during main execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f95a0b",
   "metadata": {},
   "source": [
    "#analysis if failed ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Hardcoded list of the 4 still failing video IDs\n",
    "failed_video_ids = ['XXX','YYY','ZZZ'] #put your failed hashed IDS\n",
    "\n",
    "# Load analysis_results.json\n",
    "with open('analysis_results.json', 'r', encoding='utf-8') as f:\n",
    "    analysis_results = json.load(f)\n",
    "\n",
    "# Filter entries for failed video IDs from analysis_results\n",
    "failed_analysis_results = [item for item in analysis_results if str(item.get('videoId', '')) in failed_video_ids]\n",
    "\n",
    "# Filter entries for failed video IDs from dataset\n",
    "failed_dataset_entries = dataset[dataset['videoId'].isin(failed_video_ids)].to_dict(orient='records')\n",
    "\n",
    "print(\"\\nFailed analysis_results entries:\")\n",
    "for entry in failed_analysis_results:\n",
    "    print(entry)\n",
    "\n",
    "print(\"\\nFailed dataset entries:\")\n",
    "for entry in failed_dataset_entries:\n",
    "    print(entry)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-tool",
   "language": "python",
   "name": "graph-tool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
