{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2446744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the datasets\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "#The dataset collected in January 2023\n",
    "\n",
    "file_path_jan23 = os.path.join('..', 'dataset','Compiled', 'Jan23', 'dataset_compiled_jan.json')\n",
    "with open(file_path_jan23) as result_jan23:\n",
    "    result_jan23 = json.load(result_jan23)\n",
    "    \n",
    "#The dataset collected in April 2023    \n",
    "    \n",
    "file_path_apr23 = os.path.join('..', 'dataset','Compiled', 'Apr23', 'dataset_compiled_apr.json')\n",
    "with open(file_path_apr23) as result_apr23:\n",
    "    result_apr23 = json.load(result_apr23)\n",
    "\n",
    "#The dataset collected in September 2023\n",
    "\n",
    "file_path_sep23 = os.path.join('..', 'dataset','Compiled', 'Sep23','dataset_compiled_sep.json')\n",
    "with open(file_path_sep23) as result_sep23:\n",
    "    result_sep23 = json.load(result_sep23)\n",
    "\n",
    "#The dataset collected in March 2024\n",
    "\n",
    "file_path_mar24 = os.path.join('..', 'dataset','Compiled', 'Mar24','dataset_compiled_mar.json')\n",
    "with open(file_path_mar24) as result_mar24:\n",
    "    result_mar24 = json.load(result_mar24)\n",
    "    \n",
    "#The dataset collected in Octover 2024    \n",
    "\n",
    "file_path_oct24 = os.path.join('..', 'dataset','Compiled', 'Oct24','dataset_compiled_oct.json')\n",
    "with open(file_path_oct24) as result_oct24:\n",
    "    result_oct24 = json.load(result_oct24)\n",
    "    \n",
    " #The dataset collected in January 2025    \n",
    "    \n",
    "file_path_jan25 = os.path.join('..', 'dataset','Compiled', 'Jan25','dataset_compiled.json')\n",
    "with open(file_path_jan25) as result_jan25:\n",
    "    result_jan25 = json.load(result_jan25) \n",
    "    \n",
    " #The dataset collected in April 2025    \n",
    "    \n",
    "file_path_apr25 = os.path.join('..', 'dataset','Compiled', 'Apr25','dataset_compiled.json')\n",
    "with open(file_path_apr25) as result_apr25:\n",
    "    result_apr25 = json.load(result_apr25)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the result from dictionaries with missing 'itemList' - object containing our data\n",
    "\n",
    "result_jan23 = [v for v in result_jan23 if 'itemList' in v and v['itemList']]\n",
    "\n",
    "result_apr23 = [v for v in result_apr23 if 'itemList' in v and v['itemList']]\n",
    "\n",
    "result_sep23 = [v for v in result_sep23 if 'itemList' in v and v['itemList']]\n",
    "\n",
    "result_mar24 = [v for v in result_mar24 if 'itemList' in v and v['itemList']]\n",
    "\n",
    "result_oct24 = [v for v in result_oct24 if 'itemList' in v and v['itemList']]\n",
    "\n",
    "result_jan25 = [v for v in result_jan25 if 'itemList' in v and v['itemList']]\n",
    "\n",
    "result_apr25 = [v for v in result_apr25 if 'itemList' in v and v['itemList']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b745b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty print, check the structure of the object\n",
    "\n",
    "#AIGCDescription object begin with \n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result_apr25[1]['itemList'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc61689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding nestled dictionaries for all three datasets\n",
    "\n",
    "extended_list_jan23 = []\n",
    "extended_list_apr23 = []\n",
    "extended_list_sep23 = []\n",
    "extended_list_mar24 = []\n",
    "extended_list_oct24 = []\n",
    "extended_list_jan25 = []\n",
    "extended_list_apr25 = []\n",
    "\n",
    "for i, v in enumerate(result_jan23):\n",
    "    itemlist_jan23 = v.get('itemList')\n",
    "    if itemlist_jan23:\n",
    "        extended_list_jan23.extend(itemlist_jan23)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}')\n",
    "\n",
    "for i, v in enumerate(result_apr23):\n",
    "    itemlist_apr23 = v.get('itemList')\n",
    "    if itemlist_apr23:\n",
    "        extended_list_apr23.extend(itemlist_apr23)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}')  \n",
    "        \n",
    "\n",
    "for i, v in enumerate(result_sep23):\n",
    "    itemlist_sep23 = v.get('itemList')\n",
    "    if itemlist_sep23:\n",
    "        extended_list_sep23.extend(itemlist_sep23)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}') \n",
    "        \n",
    "for i, v in enumerate(result_mar24):\n",
    "    itemlist_mar24 = v.get('itemList')\n",
    "    if itemlist_mar24:\n",
    "        extended_list_mar24.extend(itemlist_mar24)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}')\n",
    "\n",
    "for i, v in enumerate(result_oct24):\n",
    "    itemlist_oct24 = v.get('itemList')\n",
    "    if itemlist_oct24:\n",
    "        extended_list_oct24.extend(itemlist_oct24)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}')      \n",
    "\n",
    "for i, v in enumerate(result_jan25):\n",
    "    itemlist_jan25 = v.get('itemList')\n",
    "    if itemlist_jan25:\n",
    "        extended_list_jan25.extend(itemlist_jan25)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}')\n",
    "\n",
    "for i, v in enumerate(result_apr25):\n",
    "    itemlist_apr25 = v.get('itemList')\n",
    "    if itemlist_apr25:\n",
    "        extended_list_apr25.extend(itemlist_apr25)\n",
    "    else:\n",
    "        print(f'itemlist missing: {i}')             \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc81b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a non-expiring link the the video\n",
    "videolink_jan23 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_jan23]\n",
    "videolink_apr23 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_apr23]\n",
    "videolink_sep23 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_sep23]\n",
    "videolink_mar24 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_mar24]\n",
    "videolink_oct24 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_oct24]\n",
    "videolink_jan25 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_jan25]\n",
    "videolink_apr25 = ['https://www.tiktok.com/@'+v['author']['id']+'/video/'+ v['video']['id'] for v in extended_list_apr25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def expand_stickers_on_item(extended_lists):\n",
    "    \"\"\"\n",
    "    Expands the stickersOnItem data for each item in the provided lists.\n",
    "    \"\"\"\n",
    "    expanded_data = {}\n",
    "    for key, extended_list in extended_lists.items():\n",
    "        stickers_on_item = [item.get('stickersOnItem', []) for item in extended_list]\n",
    "        expanded_data[key] = stickers_on_item\n",
    "    return expanded_data\n",
    "\n",
    "def process_stickers(extended_list):\n",
    "    \"\"\"\n",
    "    Process stickers for a single extended list.\n",
    "    \"\"\"\n",
    "    sticker_data = []\n",
    "    for item in extended_list:\n",
    "        item_id = str(item['id'])\n",
    "        if 'stickersOnItem' in item:\n",
    "            for sticker in item['stickersOnItem']:\n",
    "                sticker_data.append({\n",
    "                    'id': item_id,\n",
    "                    'stickerText': sticker.get('stickerText'),\n",
    "                    'stickerType': str(sticker.get('stickerType'))\n",
    "                })\n",
    "        else:\n",
    "            sticker_data.append({'id': item_id, 'stickerText': None, 'stickerType': None})\n",
    "    return pd.DataFrame(sticker_data)\n",
    "\n",
    "def process_sticker_dataframe(df_stickers):\n",
    "    \"\"\"\n",
    "    Process a sticker DataFrame to create the pivoted structure.\n",
    "    \"\"\"\n",
    "    # Handle NaN values and ensure string type\n",
    "    df_stickers['stickerText'] = df_stickers['stickerText'].apply(\n",
    "        lambda x: ', '.join(x) if isinstance(x, list) else str(x) if pd.notna(x) else ''\n",
    "    )\n",
    "    \n",
    "    # Filter out empty strings before grouping\n",
    "    filtered = df_stickers[df_stickers['stickerText'].str.strip().astype(bool)]\n",
    "    \n",
    "    # Group by 'id' and aggregate stickerText into a list\n",
    "    grouped = filtered.groupby('id')['stickerText'].agg(list).reset_index()\n",
    "    \n",
    "    # Create concatenated_stickerText column\n",
    "    grouped['concatenated_stickerText'] = grouped['stickerText'].apply(\n",
    "        lambda x: ', '.join([str(i) for i in x if pd.notna(i) and str(i).strip()])\n",
    "    )\n",
    "    \n",
    "    # Remove rows with empty concatenated text\n",
    "    result = grouped[grouped['concatenated_stickerText'].str.strip().astype(bool)]\n",
    "    \n",
    "    return result[['id', 'concatenated_stickerText']].reset_index(drop=True)\n",
    "\n",
    "# Define your extended_lists dictionary\n",
    "extended_lists = {\n",
    "    'jan23': extended_list_jan23,\n",
    "    'apr23': extended_list_apr23,\n",
    "    'sep23': extended_list_sep23,\n",
    "    'mar24': extended_list_mar24,\n",
    "    'oct24': extended_list_oct24,\n",
    "    'jan25': extended_list_jan25,\n",
    "    'apr25': extended_list_apr25\n",
    "}\n",
    "\n",
    "# Process all datasets\n",
    "results_stickers = {}\n",
    "for key, extended_list in extended_lists.items():\n",
    "    df_stickers = process_stickers(extended_list)\n",
    "    pivoted_stickers = process_sticker_dataframe(df_stickers)\n",
    "    results_stickers[key] = pivoted_stickers\n",
    "\n",
    "# Print results\n",
    "for month, df in results_stickers.items():\n",
    "    print(f\"\\nPivoted StickerOnItem for {month}:\")\n",
    "    print(df.head())\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5db672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_extra(extended_list):\n",
    "    \"\"\"\n",
    "    Process textExtra (hashtags) for a single extended list.\n",
    "    \"\"\"\n",
    "    textExtra = []\n",
    "    for v in extended_list:\n",
    "        item_id = str(v['id'])\n",
    "        if 'textExtra' in v and v['textExtra']:\n",
    "            for hashtag_entry in v['textExtra']:\n",
    "                textExtra.append({\n",
    "                    'id': item_id,\n",
    "                    'hashtagName': str(hashtag_entry.get('hashtagName', '')),\n",
    "                    'hashtagId': str(hashtag_entry.get('hashtagId', ''))\n",
    "                })\n",
    "        else:\n",
    "            textExtra.append({'id': item_id, 'hashtagName': '', 'hashtagId': ''})\n",
    "    \n",
    "    return pd.DataFrame(textExtra)\n",
    "\n",
    "def process_textExtra_dataframe(df_textExtra):\n",
    "    \"\"\"\n",
    "    Process a textExtra DataFrame to create the pivoted structure.\n",
    "    \"\"\"\n",
    "    # Filter out empty hashtagNames\n",
    "    df_filtered = df_textExtra[df_textExtra['hashtagName'].notna() & (df_textExtra['hashtagName'] != '')]\n",
    "    \n",
    "    # Group by 'id' and aggregate hashtagNames into a list\n",
    "    grouped = df_filtered.groupby('id')['hashtagName'].agg(list).reset_index()\n",
    "    \n",
    "    # Create concatenated_hashtagName column\n",
    "    grouped['concatenated_hashtagName'] = grouped['hashtagName'].apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    return grouped[['id', 'concatenated_hashtagName']]\n",
    "\n",
    "# Define your extended_lists dictionary\n",
    "extended_lists = {\n",
    "    'jan23': extended_list_jan23,\n",
    "    'apr23': extended_list_apr23,\n",
    "    'sep23': extended_list_sep23,\n",
    "    'mar24': extended_list_mar24,\n",
    "    'oct24': extended_list_oct24,\n",
    "    'jan25': extended_list_jan25,\n",
    "    'apr25': extended_list_apr25\n",
    "}\n",
    "\n",
    "# Process all datasets\n",
    "results_hashtags = {}\n",
    "for key, extended_list in extended_lists.items():\n",
    "    df_textExtra = process_text_extra(extended_list)\n",
    "    pivoted_textExtra = process_textExtra_dataframe(df_textExtra)\n",
    "    results_hashtags[key] = pivoted_textExtra\n",
    "\n",
    "# Print results\n",
    "for month, df in results_hashtags.items():\n",
    "    print(f\"\\nPivoted TextExtra for {month}:\")\n",
    "    print(df.head())\n",
    "    print(f\"Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424352e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "def create_monthly_dataframe(extended_list, videolink, results_hashtags, results_stickers):\n",
    "    \"\"\"\n",
    "    Create a dataframe for each month's dataset.\n",
    "    \"\"\"\n",
    "    # Create a base dataframe from the extended list\n",
    "    df = pd.DataFrame(extended_list)\n",
    "    df_videolink = pd.DataFrame(videolink, columns=['videoLink'])\n",
    "    \n",
    "    # Join video links\n",
    "    df = df.join(df_videolink)\n",
    "    \n",
    "    # Merge hashtags and stickers data\n",
    "    df = df.merge(results_hashtags, on='id', how='left')\n",
    "    df = df.merge(results_stickers, on='id', how='left')\n",
    "\n",
    "    # Select and expand objects from the list and convert datetime of the content creation\n",
    "    df['authorId'] = df.author.apply(lambda x: x.get('uniqueId'))\n",
    "    df['commentCount'] = df.stats.apply(lambda x: x.get('commentCount'))\n",
    "    df['dateTime'] = df.createTime.apply(datetime.fromtimestamp)\n",
    "    df['desc'] = df.desc\n",
    "    df['diggCount'] = df.stats.apply(lambda x: x.get('diggCount'))\n",
    "    df['duetEnabled'] = df.duetEnabled\n",
    "    df['duetFromId'] = df.duetInfo.apply(lambda x: x.get('duetFromId') if x else None)\n",
    "    df['hashtagNames'] = df.concatenated_hashtagName\n",
    "    df['stickersText'] = df.concatenated_stickerText\n",
    "    df['musicAlbum'] = df.music.apply(lambda x: x.get('album'))\n",
    "    df['musicAuthorName'] = df.music.apply(lambda x: x.get('authorName'))\n",
    "    df['musicId'] = df.music.apply(lambda x: x.get('id'))\n",
    "    df['musicTitle'] = df.music.apply(lambda x: x.get('title'))\n",
    "    df['playCount'] = df.stats.apply(lambda x: x.get('playCount'))\n",
    "    df['shareCount'] = df.stats.apply(lambda x: x.get('shareCount'))\n",
    "    df['videoId'] = df.id\n",
    "    df['videoLink'] = df.videoLink\n",
    "\n",
    "    # Select only the required columns\n",
    "    columns_to_keep = [\n",
    "        'authorId', 'commentCount', 'dateTime', 'desc', 'diggCount', 'duetEnabled', 'duetFromId',\n",
    "        'hashtagNames', 'stickersText', 'musicAlbum', 'musicAuthorName', 'musicId', 'musicTitle',\n",
    "        'playCount', 'shareCount', 'videoId', 'videoLink'\n",
    "    ]\n",
    "    \n",
    "    return df[columns_to_keep]\n",
    "\n",
    "# Dictionary of months and their corresponding data\n",
    "months_data = {\n",
    "    'January23': (extended_list_jan23, videolink_jan23, results_hashtags['jan23'], results_stickers['jan23']),\n",
    "    'April23': (extended_list_apr23, videolink_apr23, results_hashtags['apr23'], results_stickers['apr23']),\n",
    "    'September23': (extended_list_sep23, videolink_sep23, results_hashtags['sep23'], results_stickers['sep23']),\n",
    "    'March24': (extended_list_mar24, videolink_mar24, results_hashtags['mar24'], results_stickers['mar24']),\n",
    "    'October24': (extended_list_oct24, videolink_oct24, results_hashtags['oct24'], results_stickers['oct24']),\n",
    "    'January25': (extended_list_jan25, videolink_jan25, results_hashtags['jan25'], results_stickers['jan25']),\n",
    "    'April25': (extended_list_apr25, videolink_apr25, results_hashtags['apr25'], results_stickers['apr25']),\n",
    "}\n",
    "\n",
    "# Process each month's data and store the results\n",
    "monthly_dataframes = {}\n",
    "for month, data in months_data.items():\n",
    "    monthly_dataframes[month] = create_monthly_dataframe(*data)\n",
    "\n",
    "# Print the columns and shape of each monthly dataframe\n",
    "for month, df in monthly_dataframes.items():\n",
    "    print(f\"\\nColumns for {month} dataframe:\")\n",
    "    pprint(df.columns.tolist())\n",
    "    print(f\"Shape of {month} dataframe: {df.shape}\")\n",
    "\n",
    "# You can access individual dataframes like this:\n",
    "# df_jan23 = monthly_dataframes['January23']\n",
    "# df_apr23 = monthly_dataframes['April23']\n",
    "# ... and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_unique_videos_to_csv(monthly_dataframes, output_folder='../csv'):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for month, df in monthly_dataframes.items():\n",
    "        # Get the unique 'videoId' rows\n",
    "        df_unique = df.drop_duplicates(subset='videoId')\n",
    "\n",
    "        # Create the filename\n",
    "        filename = f\"Video_df_{month.lower()}.csv\"\n",
    "        filepath = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Save to CSV file\n",
    "        df_unique.to_csv(filepath, index=False)\n",
    "        \n",
    "        print(f\"Saved {month} dataset with {len(df_unique)} unique videos to {filepath}\")\n",
    "\n",
    "# Usage\n",
    "save_unique_videos_to_csv(monthly_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41eb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Combine all monthly dataframes into one (ensure monthly_dataframes exists)\n",
    "all_months_df = pd.concat(monthly_dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Remove duplicate video IDs (keep first occurrence)\n",
    "unique_videos_df = all_months_df.drop_duplicates(subset='videoId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_videos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ee125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = '../csv'\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the filename and filepath\n",
    "filename = \"unique_videos_df.csv\"\n",
    "filepath = os.path.join(output_folder, filename)\n",
    "\n",
    "# Save the filtered_df DataFrame to CSV\n",
    "try:\n",
    "    unique_videos_df.to_csv(filepath, index=False)\n",
    "    print(f\"Filtered DataFrame successfully saved to {filepath}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79adfc07",
   "metadata": {},
   "source": [
    "**Basic Statistics and Visualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c286e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_top_contributors(combined_df, top_n=10, by_month=False):\n",
    "    \"\"\"\n",
    "    Visualize top contributors by video count with enhanced styling and metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - combined_df: DataFrame containing all video data\n",
    "    - top_n: Number of top contributors to display\n",
    "    - by_month: Whether to split visualization by month (True/False)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 10 if by_month else 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    palette = sns.color_palette(\"husl\", n_colors=top_n)\n",
    "\n",
    "    if by_month:\n",
    "        # Create a time-based grouping\n",
    "        combined_df['month'] = combined_df['dateTime'].dt.to_period('M')\n",
    "        g = sns.FacetGrid(combined_df, col='month', col_wrap=3, height=6, aspect=1.2)\n",
    "        g.map_dataframe(lambda data, **kws: \n",
    "            data.groupby('authorId')\n",
    "               .agg(\n",
    "                   videos=('videoId', 'nunique'),\n",
    "                   avg_plays=('playCount', 'mean'),\n",
    "                   total_shares=('shareCount', 'sum')\n",
    "                )\n",
    "               .nlargest(top_n, 'videos')\n",
    "               .plot(kind='barh', width=0.85, **kws)\n",
    "        )\n",
    "        g.set_titles(\"Month: {col_name}\")\n",
    "        g.set_axis_labels(\"Number of Videos\", \"Author ID\")\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.fig.suptitle(f'Top {top_n} Contributors by Month', fontsize=16)\n",
    "        \n",
    "    else:\n",
    "        # Aggregate overall statistics\n",
    "        author_stats = combined_df.groupby('authorId').agg(\n",
    "            videos=('videoId', 'nunique'),\n",
    "            avg_plays=('playCount', 'mean'),\n",
    "            total_shares=('shareCount', 'sum')\n",
    "        ).nlargest(top_n, 'videos')\n",
    "\n",
    "        # Create combined metric plot\n",
    "        fig, ax1 = plt.subplots(figsize=(18, 10))\n",
    "        \n",
    "        # Bar plot for video count\n",
    "        author_stats['videos'].plot(kind='barh', ax=ax1, color=palette, width=0.7)\n",
    "        ax1.set_xlabel('Number of Videos')\n",
    "        ax1.set_ylabel('Author ID')\n",
    "        ax1.set_title(f'Top {top_n} Contributors (Video Count)', pad=20)\n",
    "        \n",
    "        # Add annotations\n",
    "        for i, (idx, row) in enumerate(author_stats.iterrows()):\n",
    "            ax1.text(row['videos'] + 0.5, i, \n",
    "                    f\"{row['videos']} videos\\n\"\n",
    "                    f\"Avg plays: {row['avg_plays']:,.0f}\\n\"\n",
    "                    f\"Total shares: {row['total_shares']:,.0f}\",\n",
    "                    va='center', ha='left', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1. Overall top contributors\n",
    "visualize_top_contributors(unique_videos_df, top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Create a copy to avoid modifying the original dataframe\n",
    "plot_df = unique_videos_df.copy()\n",
    "\n",
    "# Convert datetime using proper pandas methods\n",
    "plot_df = plot_df.assign(\n",
    "    date=pd.to_datetime(plot_df['dateTime']),\n",
    "    Quarture =lambda x: x['date'].dt.to_period('Q')\n",
    ")\n",
    "\n",
    "# Group and count\n",
    "grouped_data = plot_df.groupby('Quarture')['videoId'].count()\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "grouped_data.plot(kind='line', marker='o', ax=ax, linewidth=2)\n",
    "ax.set_title('Video Publication Timeline', fontsize=14, pad=20)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Video Count', fontsize=12)\n",
    "\n",
    "# Format x-ticks\n",
    "ax.set_xticks(grouped_data.index)\n",
    "ax.set_xticklabels(\n",
    "    [pd.to_datetime(str(x)).strftime('%b\\n%Y') for x in grouped_data.index],\n",
    "    rotation=0,\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "# Add data labels using zip to avoid Period type issues\n",
    "for date, count in zip(grouped_data.index, grouped_data.values):\n",
    "    ax.text(\n",
    "        pd.to_datetime(str(date)),\n",
    "        count + 0.5,\n",
    "        f'{count}',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select videos with specific hahstags present\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_and_save_top_videos(df, hashtags, min_play_count, top_n, output_folder='../csv'):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    columns_to_display = ['videoId', 'videoLink', 'dateTime', 'desc', 'hashtagNames', 'stickersText', 'musicTitle', 'diggCount', 'playCount', 'shareCount', 'commentCount']\n",
    "\n",
    "    for hashtag in hashtags:\n",
    "        # Filter the dataframe\n",
    "        filtered_df = df[\n",
    "            (df['hashtagNames'].str.contains(hashtag, case=False, na=False)) &\n",
    "            (df['playCount'] > min_play_count)\n",
    "        ]\n",
    "\n",
    "        # Sort by diggCount in descending order and get unique videoId rows\n",
    "        top_videos = (\n",
    "            filtered_df\n",
    "            .sort_values('diggCount', ascending=False)\n",
    "            .drop_duplicates('videoId')\n",
    "        )\n",
    "\n",
    "        # Get the actual number of videos meeting the criteria\n",
    "        actual_top_n = min(len(top_videos), top_n)\n",
    "\n",
    "        if actual_top_n > 0:\n",
    "            top_videos_display = top_videos.head(actual_top_n)[columns_to_display]\n",
    "\n",
    "            # Display the results\n",
    "            print(f\"\\nTop {actual_top_n} '{hashtag}' videos with over {min_play_count:,} play count:\")\n",
    "            print(top_videos_display.to_string(index=False))\n",
    "\n",
    "            # Print the total number of matching videos\n",
    "            print(f\"Total number of matching videos: {len(top_videos)}\")\n",
    "\n",
    "            # Save to CSV only if there are videos to save\n",
    "            filename = f\"top_{actual_top_n}_{hashtag}_videos.csv\"\n",
    "            filepath = os.path.join(output_folder, filename)\n",
    "            top_videos_display.to_csv(filepath, index=False)\n",
    "            print(f\"Saved {actual_top_n} videos to {filepath}\")\n",
    "        else:\n",
    "            print(f\"\\nNo videos found for '{hashtag}' with over {min_play_count:,} play count.\")\n",
    "\n",
    "# Usage\n",
    "df_oct = monthly_dataframes['March']\n",
    "hashtags_to_process = ['bali', 'portugal', 'thailand', 'mexico']\n",
    "min_play_count = 10000\n",
    "top_n = 10\n",
    "\n",
    "process_and_save_top_videos(df_oct, hashtags_to_process, min_play_count, top_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
