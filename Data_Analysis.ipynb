{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac215f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# --- Data Cleaning and Validation Functions ---\n",
    "\n",
    "def extract_codes_deep(codes):\n",
    "    \"\"\"\n",
    "    Recursively extracts and flattens codes from various nested formats.\n",
    "    Handles lists, dictionaries, and string-encoded lists/dictionaries.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if isinstance(codes, list):\n",
    "        for item in codes:\n",
    "            result.extend(extract_codes_deep(item))\n",
    "    elif isinstance(codes, dict):\n",
    "        if 'code' in codes and isinstance(codes['code'], str):\n",
    "            result.append(codes['code'])\n",
    "    elif isinstance(codes, str):\n",
    "        stripped_code = codes.strip()\n",
    "        if (stripped_code.startswith('[') and stripped_code.endswith(']')) or \\\n",
    "           (stripped_code.startswith('{') and stripped_code.endswith('}')):\n",
    "            try:\n",
    "                parsed_code = ast.literal_eval(stripped_code)\n",
    "                result.extend(extract_codes_deep(parsed_code))\n",
    "            except (ValueError, SyntaxError):\n",
    "                if re.match(r\"^[A-Z]{2,}-[A-Z]{2,}$\", stripped_code):\n",
    "                    result.append(stripped_code)\n",
    "        elif re.match(r\"^[A-Z]{2,}-[A-Z]{2,}$\", stripped_code):\n",
    "            result.append(stripped_code)\n",
    "    return result\n",
    "\n",
    "def clean_entry_and_capture_failures(entry, failed_ids_list):\n",
    "    \"\"\"\n",
    "    Validates and restructures a single JSON object. If validation fails,\n",
    "    it adds the hashed_videoId to a list of failed IDs.\n",
    "    \n",
    "    Returns a clean dictionary if the entry is valid, otherwise returns None.\n",
    "    \"\"\"\n",
    "    # Attempt to get hashed_videoId early for logging purposes\n",
    "    video_id = entry.get('hashed_videoId') if isinstance(entry, dict) else None\n",
    "    \n",
    "    # 1. Check for the presence of essential keys\n",
    "    if not isinstance(entry, dict) or not all(k in entry for k in ['codes', 'reason', 'hashed_videoId']):\n",
    "        if video_id:\n",
    "            failed_ids_list.append(video_id)\n",
    "        return None\n",
    "        \n",
    "    # 2. Extract and validate codes\n",
    "    codes_extracted = extract_codes_deep(entry['codes'])\n",
    "    if not codes_extracted:\n",
    "        if video_id:\n",
    "            failed_ids_list.append(video_id)\n",
    "        return None\n",
    "        \n",
    "    # 3. Validate reason and hashed_videoId\n",
    "    reason = entry.get('reason')\n",
    "    if not isinstance(reason, str) or not reason.strip() or not video_id:\n",
    "        if video_id:\n",
    "            failed_ids_list.append(video_id)\n",
    "        return None\n",
    "        \n",
    "    # 4. Construct the clean object\n",
    "    return {\n",
    "        'codes': sorted(list(set(codes_extracted))),\n",
    "        'reason': reason.strip(),\n",
    "        'hashed_videoId': video_id\n",
    "    }\n",
    "\n",
    "# --- Main Data Loading and Processing Pipeline ---\n",
    "\n",
    "# 1. Specify file paths\n",
    "result_file_path = 'your_results.json'\n",
    "dataset_path = '../your_dataset.csv'\n",
    "\n",
    "# 2. Initialize lists for cleaned data and failed IDs\n",
    "cleaned_data = []\n",
    "failed_video_ids = []\n",
    "results_df = None\n",
    "\n",
    "# 3. Load and clean the JSON data\n",
    "try:\n",
    "    with open(result_file_path, 'r') as file:\n",
    "        raw_data = json.load(file)\n",
    "    print(f\"Successfully loaded {result_file_path} with {len(raw_data)} records.\")\n",
    "    \n",
    "    # Apply the cleaning function to each entry in the raw data\n",
    "    cleaned_data = [clean_entry_and_capture_failures(e, failed_video_ids) for e in raw_data]\n",
    "    cleaned_data = [item for item in cleaned_data if item is not None] # Remove None entries\n",
    "    \n",
    "    # Remove duplicates from failed_video_ids\n",
    "    failed_video_ids = sorted(list(set(failed_video_ids)))\n",
    "    \n",
    "    if not cleaned_data:\n",
    "        raise ValueError(\"No valid data remaining after cleaning. Check JSON file structure.\")\n",
    "\n",
    "    results_df = pd.DataFrame(cleaned_data)\n",
    "    print(f\"Data cleaning complete.\")\n",
    "    print(f\"  - {len(results_df)} valid records processed.\")\n",
    "    print(f\"  - {len(failed_video_ids)} records failed validation and were skipped.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {result_file_path} was not found. Please check the file path.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: The file {result_file_path} is not a valid JSON. Please check its content.\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# 4. Load external dataset and enrich the cleaned data\n",
    "if results_df is not None:\n",
    "    try:\n",
    "        dataset = pd.read_csv(dataset_path)\n",
    "        print(f\"Successfully loaded {dataset_path}.\")\n",
    "        \n",
    "        # Merge the cleaned data with the dataset to add 'dateTime'\n",
    "        if 'hashed_videoId' in dataset.columns:\n",
    "            enriched_data = pd.merge(results_df, dataset[['hashed_videoId', 'dateTime']], on='hashed_videoId', how='left')\n",
    "            print(\"Cleaned data enriched with 'dateTime' column.\")\n",
    "            print(\"\\n--- Sample of Final Enriched Data ---\")\n",
    "            print(enriched_data.head())\n",
    "            \n",
    "            # You can now inspect or save the list of failed video IDs\n",
    "            print(f\"\\n--- {len(failed_video_ids)} Failed Video IDs Captured ---\")\n",
    "            # print(failed_video_ids) # Uncomment to display the list of failed IDs\n",
    "            \n",
    "            # Optionally, save the failed IDs to a file\n",
    "            # with open('failed_video_ids.txt', 'w') as f:\n",
    "            #     for video_id in failed_video_ids:\n",
    "            #         f.write(f\"{video_id}\\\\n\")\n",
    "            # print(\"Failed video IDs saved to failed_video_ids.txt\")\n",
    "\n",
    "        else:\n",
    "            print(\"Error: 'hashed_videoId' column not found in the CSV dataset. Cannot enrich data.\")\n",
    "            enriched_data = results_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The dataset file {dataset_path} was not found. Cannot enrich data.\")\n",
    "        enriched_data = results_df\n",
    "else:\n",
    "    print(\"Cannot proceed with data enrichment due to issues with analysis_results.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b86043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Create a copy to avoid modifying the original dataframe\n",
    "plot_df = enrich_with_datetime(result_data, dataset)\n",
    "\n",
    "# Convert datetime using proper pandas methods\n",
    "plot_df = plot_df.assign(\n",
    "    date=pd.to_datetime(plot_df['dateTime']),\n",
    "    Quarture =lambda x: x['date'].dt.to_period('Q')\n",
    ")\n",
    "# Filter the DataFrame to start from 2019Q4\n",
    "plot_df = plot_df[plot_df['Quarture'] >= 'Jan2019'].copy()\n",
    "\n",
    "# Group and count\n",
    "grouped_data = plot_df.groupby('Quarture')['hashed_videoId'].count()\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "grouped_data.plot(kind='line', marker='o', ax=ax, linewidth=2)\n",
    "ax.set_title('Video Publication Timeline', fontsize=14, pad=20)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Video Count', fontsize=12)\n",
    "\n",
    "# Format x-ticks\n",
    "ax.set_xticks(grouped_data.index)\n",
    "ax.set_xticklabels(\n",
    "    [pd.to_datetime(str(x)).strftime('%b\\n%Y') for x in grouped_data.index],\n",
    "    rotation=0,\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "# Add data labels using zip to avoid Period type issues\n",
    "for date, count in zip(grouped_data.index, grouped_data.values):\n",
    "    ax.text(\n",
    "        pd.to_datetime(str(date)),\n",
    "        count + 0.5,\n",
    "        f'{count}',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73443c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'dateTime' to datetime if not already\n",
    "enriched_data['dateTime'] = pd.to_datetime(enriched_data['dateTime'])\n",
    "\n",
    "# Assign each post to a quarter\n",
    "enriched_data['quarter'] = enriched_data['dateTime'].dt.to_period('Q')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78af8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Distribution of content tagged with each 'element'\n",
    "element_counts = merged_data['element'].value_counts(dropna=False).rename('Count')\n",
    "element_percentages = (merged_data['element'].value_counts(normalize=True, dropna=False) * 100).round(2).rename('Percentage')\n",
    "\n",
    "element_stats = pd.concat([element_counts, element_percentages], axis=1)\n",
    "print(\"Content distribution by element:\")\n",
    "print(element_stats)\n",
    "\n",
    "# 3) Distribution of 'need' within each 'element' (counts and row-wise percentages)\n",
    "# Counts cross-tab\n",
    "element_need_counts = pd.crosstab(merged_data['element'], merged_data['need'], dropna=False)\n",
    "print(\"\\nElement x Need (counts):\")\n",
    "print(element_need_counts)\n",
    "\n",
    "# Row-wise percent within element\n",
    "element_need_pct = (pd.crosstab(merged_data['element'], merged_data['need'], normalize='index', dropna=False) * 100).round(2)\n",
    "print(\"\\nElement x Need (row-wise %):\")\n",
    "print(element_need_pct)\n",
    "\n",
    "# 4) Optional: overall need distribution (useful context)\n",
    "need_counts = merged_data['need'].value_counts(dropna=False).rename('Count')\n",
    "need_percentages = (merged_data['need'].value_counts(normalize=True, dropna=False) * 100).round(2).rename('Percentage')\n",
    "need_stats = pd.concat([need_counts, need_percentages], axis=1)\n",
    "print(\"\\nOverall need distribution:\")\n",
    "print(need_stats)\n",
    "\n",
    "\n",
    "# Stacked (long) format for stacked bar charts\n",
    "element_need_stacked = (\n",
    "    element_need_counts\n",
    "    .reset_index()\n",
    "    .melt(id_vars='element', var_name='Need', value_name='Count')\n",
    ")\n",
    "# Add row-wise percentage per element\n",
    "row_totals = element_need_stacked.groupby('element')['Count'].transform('sum')\n",
    "element_need_stacked['Percentage'] = (element_need_stacked['Count'] / row_totals * 100).round(2)\n",
    "\n",
    "\n",
    "# 6) Optional: concise textual summary\n",
    "total = len(merged_data)\n",
    "print(\"\\nSummary:\")\n",
    "for el, row in element_stats.iterrows():\n",
    "    print(f\"- {el}: {int(row['Count'])} videos ({row['Percentage']}%)\")\n",
    "print(\"\\nNeed distribution within each element (%):\"\n",
    "for el, row in element_need_pct.iterrows():\n",
    "    parts = [f\"{need}={row[need]}%\" for need in element_need_pct.columns]\n",
    "    print(f\"- {el}: \" + \", \".join(parts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3093c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard scores for all co-occurring code pairs for each element and quarter\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_jaccard_scores_over_time(enriched_data, needs_df):\n",
    "    \"\"\"\n",
    "    Calculates Jaccard scores for all co-occurring code pairs for each element and quarter.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the element, quarter, code pair, and Jaccard score.\n",
    "    \"\"\"\n",
    "    # Merge to get 'element' associated with each video\n",
    "    merged_data = pd.merge(enriched_data, needs_df[['videoId', 'element']], on='videoId', how='inner')\n",
    "    \n",
    "    all_scores = []\n",
    "\n",
    "    # Group data by both element and quarter\n",
    "    for (element, quarter), group in merged_data.groupby(['element', 'quarter']):\n",
    "        \n",
    "        # --- Standard Jaccard Calculation Logic ---\n",
    "        all_codes_in_group = set()\n",
    "        for codes in group['codes']:\n",
    "            all_codes_in_group.update(codes)\n",
    "        \n",
    "        code_counts = {code: 0 for code in all_codes_in_group}\n",
    "        \n",
    "        cooccurrence_counts = {}\n",
    "\n",
    "        for codes_list in group['codes']:\n",
    "            unique_codes_in_video = set(codes_list)\n",
    "            for code in unique_codes_in_video:\n",
    "                if code in code_counts:\n",
    "                    code_counts[code] += 1\n",
    "            \n",
    "            for code1, code2 in combinations(sorted(list(unique_codes_in_video)), 2):\n",
    "                pair = tuple(sorted((code1, code2)))\n",
    "                cooccurrence_counts[pair] = cooccurrence_counts.get(pair, 0) + 1\n",
    "        \n",
    "        # Calculate Jaccard score for each pair that co-occurred\n",
    "        for (code1, code2), intersection in cooccurrence_counts.items():\n",
    "            union = code_counts.get(code1, 0) + code_counts.get(code2, 0) - intersection\n",
    "            jaccard_score = intersection / union if union > 0 else 0.0\n",
    "            \n",
    "            all_scores.append({\n",
    "                'element': element,\n",
    "                'quarter': str(quarter),\n",
    "                'code_pair': f\"{code1} & {code2}\",\n",
    "                'jaccard': jaccard_score\n",
    "            })\n",
    "            \n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca177c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_smoothed_jaccard_heatmap(jaccard_scores, target_element, rolling_window=2):\n",
    "    \"\"\"\n",
    "    Generates a heatmap of smoothed Jaccard scores for a specific element.\n",
    "    - Applies a rolling average to the scores over time.\n",
    "    - Rows are code pairs, columns are quarters.\n",
    "    \"\"\"\n",
    "    # Convert the list of scores to a DataFrame\n",
    "    scores_df = pd.DataFrame(jaccard_scores)\n",
    "    \n",
    "    # Filter for the specific element we want to plot\n",
    "    element_df = scores_df[scores_df['element'] == target_element]\n",
    "    \n",
    "    if element_df.empty:\n",
    "        print(f\"No data available to plot for element: {target_element}\")\n",
    "        return\n",
    "\n",
    "    # Pivot the data to create the base for the heatmap (code pairs vs. time)\n",
    "    heatmap_base = element_df.pivot_table(\n",
    "        index='code_pair', \n",
    "        columns='quarter', \n",
    "        values='jaccard'\n",
    "    ).fillna(0) # Fill missing pairs in a quarter with 0\n",
    "\n",
    "    # Ensure quarters are sorted chronologically\n",
    "    heatmap_base = heatmap_base.reindex(sorted(heatmap_base.columns), axis=1)\n",
    "\n",
    "    # --- Apply the rolling average to smooth the data across time (the key step) ---\n",
    "    smoothed_heatmap_df = heatmap_base.rolling(\n",
    "        window=rolling_window, \n",
    "        axis=1, # Apply rolling window across columns (time)\n",
    "        min_periods=1\n",
    "    ).mean()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(18, 10)) # Adjust size as needed\n",
    "\n",
    "    sns.heatmap(\n",
    "        smoothed_heatmap_df,\n",
    "        ax=ax,\n",
    "        cmap='YlOrRd', # A yellow-orange-red colormap similar to the example\n",
    "        linewidths=.5,\n",
    "        annot=True, # Display the Jaccard scores on the cells\n",
    "        fmt=\".2f\", # Format annotations to two decimal places\n",
    "        cbar_kws={'label': f'Jaccard Score ({rolling_window}-Quarter Rolling Avg)'}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"Smoothed Temporal Trend of Jaccard Scores for: {target_element}\", fontsize=16)\n",
    "    ax.set_xlabel(\"Time (Quarter)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Code Pair\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd1ff9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Data Processing Function from Previous Step ---\n",
    "def build_jaccard_cooccurrence_per_element(enriched_data, needs_data):\n",
    "    \"\"\"\n",
    "    Builds Jaccard co-occurrence matrices for codes, grouped by the 'element'\n",
    "    (e.g., Tourist, Migrant, Pilgrim, Worker).\n",
    "\n",
    "    Args:\n",
    "        enriched_data (pd.DataFrame): DataFrame with 'hashed_videoId' and 'codes' columns.\n",
    "        needs_data (pd.DataFrame): DataFrame with 'hashed_videoId' and 'element' columns.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are elements and values are the Jaccard matrices.\n",
    "    \"\"\"\n",
    "    merged_data = pd.merge(enriched_data, needs_data[['hashed_videoId', 'element']], on='hashed_videoId', how='inner')\n",
    "    cooccurrence_matrices = {}\n",
    "\n",
    "    for element, group in merged_data.groupby('element'):\n",
    "        all_codes = set()\n",
    "        for codes in group['codes']:\n",
    "            all_codes.update(codes)\n",
    "        all_codes_list = sorted(list(all_codes))\n",
    "        \n",
    "        if not all_codes_list:\n",
    "            continue\n",
    "\n",
    "        matrix = pd.DataFrame(0, index=all_codes_list, columns=all_codes_list, dtype=float)\n",
    "        code_counts = {code: 0 for code in all_codes_list}\n",
    "        \n",
    "        for codes_list in group['codes']:\n",
    "            unique_codes = set(codes_list)\n",
    "            for code in unique_codes:\n",
    "                if code in code_counts:\n",
    "                    code_counts[code] += 1\n",
    "            \n",
    "            for code1, code2 in combinations(unique_codes, 2):\n",
    "                if code1 in all_codes_list and code2 in all_codes_list:\n",
    "                    matrix.loc[code1, code2] += 1\n",
    "                    matrix.loc[code2, code1] += 1\n",
    "\n",
    "        for i in all_codes_list:\n",
    "            for j in all_codes_list:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                \n",
    "                intersection = matrix.loc[i, j]\n",
    "                union = code_counts[i] + code_counts[j] - intersection\n",
    "                \n",
    "                if union > 0:\n",
    "                    matrix.loc[i, j] = intersection / union\n",
    "                else:\n",
    "                    matrix.loc[i, j] = 0.0\n",
    "        \n",
    "        np.fill_diagonal(matrix.values, 0)\n",
    "        cooccurrence_matrices[element] = matrix\n",
    "        \n",
    "    return cooccurrence_matrices\n",
    "\n",
    "# --- Updated Visualization Function ---\n",
    "def plot_jaccard_heatmap(matrix, element_name):\n",
    "    \"\"\"\n",
    "    Generates and displays a heatmap for a Jaccard co-occurrence matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (pd.DataFrame): The Jaccard co-occurrence matrix.\n",
    "        element_name (str): The name of the element (e.g., 'Tourist') for the plot title.\n",
    "    \"\"\"\n",
    "    # Do not plot if the matrix is empty or too small\n",
    "    if matrix.empty or len(matrix.columns) < 2:\n",
    "        print(f\"Skipping heatmap for '{element_name}' due to insufficient data.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(matrix, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Jaccard Index'})\n",
    "    plt.title(f'Jaccard Index Co-occurrence Matrix â€“ {element_name}', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Load your data into pandas DataFrames\n",
    "# This assumes you have 'analysis_results.json' and 'needs_results.json' in the same directory.\n",
    "# Since the files are line-delimited JSON, use lines=True.\n",
    "try:\n",
    "    # 2. Generate the co-occurrence matrices per element\n",
    "    jaccard_matrices_per_element = build_jaccard_cooccurrence_per_element(enriched_data, needs_df)\n",
    "\n",
    "    # 3. Iterate through the generated matrices and plot a heatmap for each one\n",
    "    print(\"Generating heatmaps for each element...\")\n",
    "    for element, matrix in jaccard_matrices_per_element.items():\n",
    "        plot_jaccard_heatmap(matrix, element)\n",
    "    print(\"Done.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure 'analysis_results.json' and 'needs_results.json' are in the correct directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8bb16",
   "metadata": {},
   "source": [
    "Maslow Needs Projections by Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_jaccard_scores_longitudinal(enriched_data, needs_df):\n",
    "    merged_data = pd.merge(enriched_data, needs_df[['hashed_hashed_videoId', 'element']], on='hashed_hashed_videoId', how='inner')\n",
    "    records = []\n",
    "    for (element, quarter), group in merged_data.groupby(['element', 'quarter']):\n",
    "        code_counts = {}\n",
    "        code_pairs = {}\n",
    "        for codes_list in group['codes']:\n",
    "            unique_codes = set(codes_list)\n",
    "            for code in unique_codes:\n",
    "                code_counts[code] = code_counts.get(code, 0) + 1\n",
    "            for code1, code2 in combinations(sorted(unique_codes), 2):\n",
    "                pair = f\"{code1} & {code2}\"\n",
    "                code_pairs[pair] = code_pairs.get(pair, 0) + 1\n",
    "        for pair_str, intersection in code_pairs.items():\n",
    "            code1, code2 = pair_str.split(\" & \")\n",
    "            union = code_counts.get(code1, 0) + code_counts.get(code2, 0) - intersection\n",
    "            if union > 0:\n",
    "                jaccard = intersection / union\n",
    "                records.append({\n",
    "                    'element': element,\n",
    "                    'quarter': str(quarter),\n",
    "                    'code_pair': pair_str,\n",
    "                    'jaccard': jaccard\n",
    "                })\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_longitudinal_heatmap_for_distinct_pairs(jaccard_df, element, threshold=0.44, rolling_window=2):\n",
    "    # Filter for this element\n",
    "    edf = jaccard_df[jaccard_df['element'] == element]\n",
    "    if edf.empty:\n",
    "        print(f\"No data for {element}\")\n",
    "        return\n",
    "    # Identify all code pairs that ever exceed the threshold\n",
    "    mask = edf.groupby('code_pair')['jaccard'].max() > threshold\n",
    "    kept_pairs = mask[mask].index.tolist()\n",
    "    if not kept_pairs:\n",
    "        print(f\"No code pairs exceed threshold for {element}\")\n",
    "        return\n",
    "    # Filter to these code pairs, but include ALL their Jaccard values (not just those above threshold)\n",
    "    plot_df = edf[edf['code_pair'].isin(kept_pairs)]\n",
    "    # Pivot for heatmap\n",
    "    pivot = plot_df.pivot(index='code_pair', columns='quarter', values='jaccard').fillna(0)\n",
    "    pivot = pivot.reindex(sorted(pivot.columns), axis=1)\n",
    "    # Smooth with rolling mean across time axis (use .T.rolling().mean().T to avoid warnings)\n",
    "    pivot_smooth = pivot.T.rolling(window=rolling_window, min_periods=1).mean().T\n",
    "    # Keep zeros (or NaNs if you want them to be blank) for pairs never present in a given quarter\n",
    "    # (If you want blanks for missing, swap .fillna(0) for .fillna(np.nan) above)\n",
    "    plt.figure(figsize=(16, min(1+len(pivot_smooth)*0.45, 16)))\n",
    "    sns.heatmap(\n",
    "        pivot_smooth, cmap='YlOrRd', annot=True, fmt='.2f', linewidths=0.5,\n",
    "        cbar_kws={'label': f'Jaccard Score (Rolling avg, {rolling_window}q)'}\n",
    "    )\n",
    "    plt.title(f\"Most Distinct Code Co-Occurrence Trends for {element}\\n(Code pairs ever >{threshold}, all values shown)\", fontsize=15)\n",
    "    plt.xlabel(\"Quarter\")\n",
    "    plt.ylabel(\"Code Pair\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7954de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Adapted function to generate matrices with a time component ---\n",
    "def build_jaccard_matrices_over_time(enriched_data, needs_df):\n",
    "    \"\"\"\n",
    "    Builds Jaccard co-occurrence matrices for codes, grouped by element, need, and quarter.\n",
    "\n",
    "    Args:\n",
    "        enriched_data (pd.DataFrame): DataFrame with 'hashed_videoId', 'codes', and 'quarter' columns.\n",
    "        needs_df (pd.DataFrame): DataFrame with 'hashed_videoId', 'element', and 'need' columns.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tuples of (element, need, quarter) and\n",
    "              values are the corresponding Jaccard co-occurrence matrices.\n",
    "    \"\"\"\n",
    "    # Merge the dataframes to bring all necessary columns together\n",
    "    merged_data = pd.merge(enriched_data, needs_df[['hashed_videoId', 'element', 'need']], on='hashed_videoId', how='inner')\n",
    "\n",
    "    cooccurrence_matrices = {}\n",
    "\n",
    "    # Group by element, need, AND quarter\n",
    "    for (element, need, quarter), group in merged_data.groupby(['element', 'need', 'quarter']):\n",
    "        # --- The logic below is the same as your original function ---\n",
    "        all_codes = set()\n",
    "        for codes in group['codes']:\n",
    "            all_codes.update(codes)\n",
    "        all_codes_list = sorted(list(all_codes))\n",
    "\n",
    "        if not all_codes_list or len(all_codes_list) < 2:\n",
    "            continue\n",
    "\n",
    "        matrix = pd.DataFrame(0, index=all_codes_list, columns=all_codes_list, dtype=float)\n",
    "        code_counts = {code: 0 for code in all_codes_list}\n",
    "\n",
    "        for codes_list in group['codes']:\n",
    "            unique_codes = set(codes_list)\n",
    "            for code in unique_codes:\n",
    "                if code in code_counts:\n",
    "                    code_counts[code] += 1\n",
    "            for code1, code2 in combinations(unique_codes, 2):\n",
    "                if code1 in all_codes_list and code2 in all_codes_list:\n",
    "                    matrix.loc[code1, code2] += 1\n",
    "                    matrix.loc[code2, code1] += 1\n",
    "\n",
    "        for i in all_codes_list:\n",
    "            for j in all_codes_list:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                intersection = matrix.loc[i, j]\n",
    "                union = code_counts.get(i, 0) + code_counts.get(j, 0) - intersection\n",
    "                matrix.loc[i, j] = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        np.fill_diagonal(matrix.values, 0)\n",
    "        cooccurrence_matrices[(element, need, quarter)] = matrix\n",
    "        \n",
    "    return cooccurrence_matrices\n",
    "\n",
    "# --- 2. Function to plot the trends (largely the same as before) ---\n",
    "def plot_jaccard_trends_by_element(cooccurrence_matrices_over_time):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    trend_data = []\n",
    "    for (element, need, quarter), matrix in cooccurrence_matrices_over_time.items():\n",
    "        if matrix.empty or matrix.shape[0] < 2:\n",
    "            avg_score = 0\n",
    "        else:\n",
    "            num_pairs = matrix.shape[0] * (matrix.shape[1] - 1)\n",
    "            total_jaccard_sum = matrix.values.sum()\n",
    "            avg_score = total_jaccard_sum / num_pairs if num_pairs > 0 else 0\n",
    "\n",
    "        trend_data.append({\n",
    "            'quarter': quarter,\n",
    "            'need': need,\n",
    "            'element': element,\n",
    "            'average_jaccard': avg_score\n",
    "        })\n",
    "\n",
    "    if not trend_data:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    trends_df = pd.DataFrame(trend_data)\n",
    "    trends_df = trends_df.sort_values('quarter')\n",
    "    trends_df = trends_df[trends_df['quarter'] >= '2020-Q1']\n",
    "\n",
    "    elements = sorted(trends_df['element'].unique())\n",
    "    desired_order = ['Basic', 'Safety', 'Esteem', 'Social', 'Self-actualization']\n",
    "\n",
    "    num_elements = len(elements)\n",
    "    ncols, nrows = 2, 2  # Assumes 4 elements -- adjust if needed!\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 12), sharex=True, sharey=True)\n",
    "\n",
    "    for i, element in enumerate(elements):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        group_df = trends_df[trends_df['element'] == element]\n",
    "        pivot_df = group_df.pivot_table(\n",
    "            index='quarter', columns='need', values='average_jaccard', aggfunc='mean'\n",
    "        )\n",
    "\n",
    "        # Keep only columns present for this element, in the desired order\n",
    "        pivot_df = pivot_df[[col for col in desired_order if col in pivot_df.columns]]\n",
    "        rolling_avg_df = pivot_df.rolling(window=3, min_periods=1).mean()\n",
    "        # Format quarter labels as 'YYYY-Qx'\n",
    "        rolling_avg_df.index = pd.PeriodIndex(rolling_avg_df.index, freq='Q').strftime('%Y-Q%q')\n",
    "        rolling_avg_df.plot(ax=ax, marker='o')\n",
    "\n",
    "        # Dynamic subplot title with element name\n",
    "        ax.set_title(f'Average Jaccard Score Trends by Need for {element}', fontsize=14)\n",
    "        ax.set_xlabel('Quarter')\n",
    "        ax.set_ylabel('Avg. Jaccard Score')\n",
    "        ax.legend(title='Need Category', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Generate the matrices, now grouped by quarter\n",
    "jaccard_matrices_by_quarter = build_jaccard_matrices_over_time(enriched_data, needs_df)\n",
    "\n",
    "#Plot the trends from the generated data\n",
    "plot_jaccard_trends_by_element(jaccard_matrices_by_quarter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c5e55",
   "metadata": {},
   "source": [
    "#Needs by Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_jaccard_matrices_over_time(enriched_data, needs_df):\n",
    "    merged_data = pd.merge(enriched_data, needs_df[['hashed_videoId', 'element', 'need']], on='hashed_videoId', how='inner')\n",
    "    cooccurrence_matrices = {}\n",
    "    for (element, need, quarter), group in merged_data.groupby(['element', 'need', 'quarter']):\n",
    "        all_codes = set()\n",
    "        for codes in group['codes']:\n",
    "            all_codes.update(codes)\n",
    "        all_codes_list = sorted(list(all_codes))\n",
    "        if not all_codes_list or len(all_codes_list) < 2:\n",
    "            continue\n",
    "        matrix = pd.DataFrame(0, index=all_codes_list, columns=all_codes_list, dtype=float)\n",
    "        code_counts = {code: 0 for code in all_codes_list}\n",
    "        for codes_list in group['codes']:\n",
    "            unique_codes = set(codes_list)\n",
    "            for code in unique_codes:\n",
    "                if code in code_counts:\n",
    "                    code_counts[code] += 1\n",
    "            for code1, code2 in combinations(unique_codes, 2):\n",
    "                if code1 in all_codes_list and code2 in all_codes_list:\n",
    "                    matrix.loc[code1, code2] += 1\n",
    "                    matrix.loc[code2, code1] += 1\n",
    "        for i in all_codes_list:\n",
    "            for j in all_codes_list:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                intersection = matrix.loc[i, j]\n",
    "                union = code_counts.get(i, 0) + code_counts.get(j, 0) - intersection\n",
    "                matrix.loc[i, j] = intersection / union if union > 0 else 0.0\n",
    "        np.fill_diagonal(matrix.values, 0)\n",
    "        cooccurrence_matrices[(element, need, quarter)] = matrix\n",
    "    return cooccurrence_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_jaccard_trends_for_element(cooccurrence_matrices_over_time, target_element):\n",
    "    trend_data = []\n",
    "    for (element, need, quarter), matrix in cooccurrence_matrices_over_time.items():\n",
    "        if matrix.empty or matrix.shape[0] < 2:\n",
    "            avg_score = 0\n",
    "        else:\n",
    "            num_pairs = matrix.shape[0] * (matrix.shape[1] - 1)\n",
    "            total_jaccard_sum = matrix.values.sum()\n",
    "            avg_score = total_jaccard_sum / num_pairs if num_pairs > 0 else 0\n",
    "        trend_data.append({\n",
    "            'quarter': quarter,\n",
    "            'need': need,\n",
    "            'element': element,\n",
    "            'average_jaccard': avg_score\n",
    "        })\n",
    "    if not trend_data:\n",
    "        print(f\"No data available to plot for {target_element}.\")\n",
    "        return\n",
    "    trends_df = pd.DataFrame(trend_data)\n",
    "    trends_df = trends_df[(trends_df['quarter'] >= '2020-Q1') & (trends_df['element'] == target_element)]\n",
    "    trends_df = trends_df.sort_values('quarter')\n",
    "    desired_order = ['Basic', 'Safety', 'Esteem', 'Social', 'Self-actualization']\n",
    "    pivot_df = trends_df.pivot_table(\n",
    "        index='quarter', columns='need', values='average_jaccard', aggfunc='mean'\n",
    "    )\n",
    "    pivot_df = pivot_df[[col for col in desired_order if col in pivot_df.columns]]\n",
    "    rolling_avg_df = pivot_df.rolling(window=3, min_periods=1).mean()\n",
    "    rolling_avg_df.index = pd.PeriodIndex(rolling_avg_df.index, freq='Q').strftime('%Y-Q%q')\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    rolling_avg_df.plot(ax=ax, marker='o')\n",
    "    ax.set_title(f'Rolling Average Jaccard Score Trends by Need for {target_element}', fontsize=16)\n",
    "    ax.set_xlabel('Quarter')\n",
    "    ax.set_xticks(range(len(rolling_avg_df.index)))\n",
    "    ax.set_xticklabels(rolling_avg_df.index, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Average Jaccard Score')\n",
    "    ax.legend(title='Need Category', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your actual data variables for enriched_data and needs_df\n",
    "cooccurrence_matrices_over_time = build_jaccard_matrices_over_time(enriched_data, needs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d8657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elements = sorted({el for (el, _, _) in cooccurrence_matrices_over_time.keys()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f741351",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements:\n",
    "    plot_jaccard_trends_for_element(cooccurrence_matrices_over_time, element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c305642",
   "metadata": {},
   "source": [
    "This script defines a function that takes the dictionary of smoothed heatmap data, calculates the average Jaccard score for each need category across all time periods, and generates a line chart to visualize these trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38628e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assume 'merged_data' is the DataFrame you have loaded into your environment.\n",
    "\n",
    "def calculate_average_jaccard_for_period(data_period):\n",
    "    \"\"\"\n",
    "    Calculates the average Jaccard score ONLY for pairs of codes that\n",
    "    actually appear together in a given dataframe slice (quarter).\n",
    "    \"\"\"\n",
    "    all_codes_in_period = sorted(list(set(code for codes_list in data_period['codes'] for code in codes_list)))\n",
    "\n",
    "    if len(all_codes_in_period) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    intersections = {}\n",
    "    for code1, code2 in combinations(all_codes_in_period, 2):\n",
    "        intersection_count = sum(1 for codes in data_period['codes'] if code1 in codes and code2 in codes)\n",
    "        intersections[(code1, code2)] = intersection_count\n",
    "\n",
    "    code_counts = {code: sum(1 for codes in data_period['codes'] if code in codes) for code in all_codes_in_period}\n",
    "\n",
    "    jaccard_scores = []\n",
    "    for (code1, code2), intersection in intersections.items():\n",
    "        # Only calculate score for pairs that co-occur (intersection > 0)\n",
    "        if intersection > 0:\n",
    "            union = code_counts.get(code1, 0) + code_counts.get(code2, 0) - intersection\n",
    "            score = intersection / union if union > 0 else 0.0\n",
    "            jaccard_scores.append(score)\n",
    "\n",
    "    # Return the average of the scores from co-occurring pairs.\n",
    "    return np.mean(jaccard_scores) if jaccard_scores else 0.0\n",
    "\n",
    "def generate_smoothed_needs_trends(merged_data, freq='QE', window_size=2, start_date='2020-01-01'):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame of smoothed temporal trends for each need category\n",
    "    using a trailing rolling average.\n",
    "    \"\"\"\n",
    "    merged_data['dateTime'] = pd.to_datetime(merged_data['dateTime'])\n",
    "    data = merged_data[merged_data['dateTime'] >= start_date].copy()\n",
    "\n",
    "    needs = sorted(data['need'].dropna().unique())\n",
    "    trends_data = {need: {} for need in needs}\n",
    "\n",
    "    data.set_index('dateTime', inplace=True)\n",
    "    grouped_by_period = data.groupby(pd.Grouper(freq=freq))\n",
    "\n",
    "    for period_end, period_group in grouped_by_period:\n",
    "        if period_group.empty: continue\n",
    "        period_label = f\"{period_end.year}-Q{period_end.quarter}\"\n",
    "        for need in needs:\n",
    "            need_period_data = period_group[period_group['need'] == need]\n",
    "            avg_jaccard = calculate_average_jaccard_for_period(need_period_data) if not need_period_data.empty else 0.0\n",
    "            trends_data[need][period_label] = avg_jaccard\n",
    "\n",
    "    trends_df = pd.DataFrame(trends_data).fillna(0)\n",
    "    if trends_df.empty:\n",
    "        print(\"Warning: No data to plot after processing.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    trends_df.index = pd.PeriodIndex(trends_df.index, freq='Q').to_timestamp()\n",
    "    trends_df = trends_df.sort_index()\n",
    "\n",
    "    # Use a standard \"trailing\" rolling average (center=False is the default)\n",
    "    smoothed_trends_df = trends_df.rolling(window=window_size, min_periods=1).mean()\n",
    "    \n",
    "    smoothed_trends_df.index = smoothed_trends_df.index.to_period('Q').strftime('%Y-Q%q')\n",
    "    \n",
    "    return smoothed_trends_df\n",
    "\n",
    "def plot_sorted_needs_trends(trends_df, chart_title='Smoothed Temporal Need Trends No Identity'):\n",
    "    \"\"\"\n",
    "    Generates and displays a line plot with a sorted legend based on Maslow's hierarchy.\n",
    "    \"\"\"\n",
    "    if trends_df.empty:\n",
    "        print(\"Cannot plot an empty DataFrame.\")\n",
    "        return\n",
    "        \n",
    "    # --- ROBUST SORTING IMPLEMENTATION ---\n",
    "    desired_order = ['Basic', 'Safety', 'Esteem', 'Social', 'Self-actualization']\n",
    "    \n",
    "    # Get the actual needs present in the data, preserving their original casing\n",
    "    actual_needs = trends_df.columns.tolist()\n",
    "    \n",
    "    # Create a mapping from lowercase desired needs to their original casing in the data\n",
    "    actual_needs_map = {need.lower(): need for need in actual_needs}\n",
    "    \n",
    "    # Build the final list of columns in the desired order\n",
    "    ordered_columns = [actual_needs_map[d_need.lower()] for d_need in desired_order if d_need.lower() in actual_needs_map]\n",
    "\n",
    "    # Re-index the DataFrame columns to match the desired order\n",
    "    sorted_trends_df = trends_df[ordered_columns]\n",
    "\n",
    "    # Plotting\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(18, 10))\n",
    "    palette = sns.color_palette(\"viridis\", n_colors=len(sorted_trends_df.columns))\n",
    "    \n",
    "    sorted_trends_df.plot(kind='line', marker='o', ax=ax, color=palette)\n",
    "    \n",
    "    ax.set_title(chart_title, fontsize=20, pad=20)\n",
    "    ax.set_xlabel('Time (Quarter)', fontsize=14)\n",
    "    ax.set_ylabel('Average Jaccard Score (Smoothed)', fontsize=14)\n",
    "    ax.legend(title='Need Category', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Example of How to Run the Script ---\n",
    "\n",
    "# # 1. Generate the smoothed data from your 'merged_data' DataFrame\n",
    "smoothed_data = generate_smoothed_needs_trends(merged_data, window_size=2)\n",
    "#\n",
    "# 2. Plot the final chart with the sorted legend \n",
    "if not smoothed_data.empty:\n",
    "    plot_sorted_needs_trends(smoothed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa37e28",
   "metadata": {},
   "source": [
    "#Qualitative analysis of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming enriched_data, needs_df, and dataset are pre-existing DataFrames\n",
    "\n",
    "# Step 1: Merge the first two DataFrames using the correct column selection syntax\n",
    "merged_temp = pd.merge(\n",
    "    enriched_data[['videoId', 'reason', 'codes','dateTime']],\n",
    "    needs_df[['videoId', 'element', 'need','reasoning']],\n",
    "    on='videoId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 2: Merge the result of the first merge with the third DataFrame\n",
    "# Note: We select 'videoId' from 'dataset' to perform the merge\n",
    "merged_data = pd.merge(\n",
    "    merged_temp,\n",
    "    dataset[['videoId', 'videoLink']],\n",
    "    on='videoId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Define the target videoId\n",
    "target_video_id = 7312832352240028974\n",
    "\n",
    "# Select the row matching the target videoId\n",
    "row = merged_data[merged_data['videoId'] == target_video_id]\n",
    "\n",
    "if not row.empty:\n",
    "    # Extract the relevant qualitative assessment fields\n",
    "    # Note: 'reasoning' was not in the original merge. If this column exists\n",
    "    # in one of the source DataFrames, you should add it to the merge list.\n",
    "    result = {\n",
    "        'videoLink': row.iloc[0].get('videoLink', ''),\n",
    "        'codes': row.iloc[0].get('codes', ''),\n",
    "        'reason': row.iloc[0].get('reason', ''),\n",
    "        'element': row.iloc[0].get('element', ''),\n",
    "        'need': row.iloc[0].get('need', ''),\n",
    "        'dateTime': row.iloc[0].get('dateTime', ''),\n",
    "        'reasoning': row.iloc[0].get('reasoning', '') # This might return empty if not merged\n",
    "    }\n",
    "    # Display the results\n",
    "    from pprint import pprint\n",
    "    pprint(result)\n",
    "else:\n",
    "    print(f\"No entry found for videoId {target_video_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cda5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping over dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'dateTime' to pandas datetime type\n",
    "merged_data['dateTime'] = pd.to_datetime(merged_data['dateTime'])\n",
    "\n",
    "# Create a period column denoting the Year-Quarter (e.g., '2021Q1')\n",
    "merged_data['quarter'] = merged_data['dateTime'].dt.to_period('Q')\n",
    "\n",
    "# Define target quarter as a string\n",
    "target_quarter = '2023-Q1'\n",
    "\n",
    "# Filter for the target quarter and (optionally) element\n",
    "target_element = 'Migrant'\n",
    "filtered_rows = merged_data[\n",
    "    (merged_data['quarter'] == target_quarter) &\n",
    "    (merged_data['element'] == target_element)\n",
    "]\n",
    "\n",
    "if not filtered_rows.empty:\n",
    "    # Example output for the first matched row\n",
    "    for idx, row in filtered_rows.iterrows():\n",
    "        print({\n",
    "        'videoLink': row.get('videoLink', ''),\n",
    "        'codes': row.get('codes', ''),\n",
    "        'reason': row.get('reason', ''),\n",
    "        'element': row.get('element', ''),\n",
    "        'need': row.get('need', ''),\n",
    "        'dateTime': row.get('dateTime', ''),\n",
    "        'reasoning': row.get('reasoning', ''),\n",
    "        'quarter': row.get('quarter', '')\n",
    "    })\n",
    "\n",
    "else:\n",
    "    print(f\"No entries found for quarter {target_quarter} and element {target_element}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe1918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-tool",
   "language": "python",
   "name": "graph-tool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
